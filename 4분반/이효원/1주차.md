# CHIC 기계학습 스터디 1주차 정리본

_혼자 공부하는 머신러닝 + 딥러닝_ - 챕터 1 / 2 + α  
기계학습 스터디 4분반 이효원

## 인공지능 VS 머신러닝 VS 딥러닝

### 인공지능

인공지능은 인간의 지능적인 행동을 컴퓨터로 구현하려는 시도를 모두 포함하는 가장 넓은 개념이다.  
이때 인간의 지능적인 행동에는 판단, 추론, 문제 해결 등이 포함된다.  
인공지능의 목적은 알고리즘의 결과가 사람이 사고한 산출물처럼 보이게 하는 것이기에, 반드시 학습을 포함할 필요는 없다.

### 머신러닝

인공지능의 하위 분야로, 사람이 모든 규칙을 명시해주지 않아도 알고리즘이 데이터로부터 규칙을 찾아내 주어진 역할을 수행하는 방식이다.  
이때, 데이터로부터 규칙을 찾아내는 과정을 학습(적합)이라고 한다.

### 딥러닝

머신러닝 알고리즘의 한 종류로, 여러 층으로 구성된 인공 신경망을 이용한 학습 방법이다. 머신러닝에서는 사람이 중요한 특징을 직접 설계하는 경우가 많았지만, 딥러닝은 이러한 특징 추출 과정까지 모델이 스스로 수행한다.  
이미지 인식부터 음성 인식, 자연어 처리 등의 분야에서 더욱 뛰어난 성능을 보인다.

단, 더 많은 데이터와 더 많은 연산 자원을 필요로 하며 모델 내부 해석이 힘들다는 단점 또한 존재한다.

> 학습 모델의 내부 해석이 힘든 경우 이를 블랙 박스 모델이라고 부르며, 해석이 가능한 경우 이를 화이트 박스 모델 / 투명한 모델 (transparent model) / 설명 가능한 인공지능 (XAI, explainable AI)라고 부른다.

## 통계학적 관점에서 본 머신러닝의 기본 구조

머신러닝은 통계학적 관점에서 보면 표본으로부터 모집단의 특성을 나타내는 모수를 추정하는 문제로 해석할 수 있다.  
즉, 관측 가능한 데이터로부터 데이터 생성 과정을 가정하고, 그 모델을 규정하는 값(파라미터, 모수)를 추정하는 것이 핵심이다.

### 확률 변수

확률 변수는 확률적 실험의 결과를 실수에 대응시키는 함수이다.  
통계학에서는 모집단의 각 관측값을 확률 변수를 통해 나타내며, 이때 확률 변수는 값이 가질 수 있는 형태에 따라

- 이산형 확률 변수: 값이 셀 수 있는 경우 (e.g., 주사위 눈, 다트 점수)
- 연속형 확률 변수: 값이 연속적인 구간인 경우 (e.g., 키, 몸무게)

로 나뉜다.

또한, 확률 변수는 어떠한 확률 분포를 따르는데

- 이산형이면 확률질량함수(PMF, Probability Mass Function)
- 연속형이면 확률밀도함수(PDF, Probability Density Function)

로 표현한다.

e.g., 다트판에 꽂힌 다트(실험 결과) >>> 점수(실현값)

### 모집단

모집단은 우리가 궁극적으로 알고 싶은 대상 전체를 뜻하는 말이다.  
통계학에서 모집단은 어떤 확률분포를 따른다고 가정하며, 이는 모수(parameter, $\theta$)로 결정된다.

대부분의 경우 모집단 전체를 직접 관측하는 것은 불가능하므로, 모집단의 일부인 표본을 통해 모집단의 특성을 간접적으로 추론하게 된다.

### 모수

모수는 모집단이 따르는 확률분포의 형태와 특성(모양, 퍼짐의 정도 등)을 결정하는 값이다.  
이는 고정된 값이지만, 모집단 전체를 관측할 수 없기에 일반적으로 알려져있지 않은 상태이다.  
e.g., 평균 $\mu$, 분산 $\sigma^2$, 비율 $p$

따라서, 통계학의 주요 목적은 표본으로부터 모수를 추정하는 것이다.

머신러닝에서도 비슷하게, 우리가 선택한 모델이 존재하고 이를 조절하는 값이 존재한다.  
이때, 이 값을 파라미터라고 칭하며 통계학의 모수 개념과 유사하다.

### 표본

표본(sample)은 모집단으로부터 임의 추출된 관측치들의 집합이다.
표본은 확률변수 $X_1, X_2, ... ,X_n$의 실현값으로 구성되며, 실제로 우리가 관측 가능한 데이터이다.

표본 추출(sampling)은 확률적 과정이므로 표본 자체는 무작위성을 가진다.  
즉, 같은 모집단에서 동일한 방법으로 표본을 다시 추출해도 데이터가 달라질 가능성이 있고, 이에 따라 추정량도 달라진다.

### 추정

추정(estimation)은 표본을 이용하여 모집단의 모수를 추론하는 과정이다.  
표본으로부터 계산된 함수형태의 값을 추정량(estimator)이라 하며, 일반적으로 다음과 같이 표현된다.

$$\hat\theta = f(\text{sample observations})$$

> **추정량(estimator) VS 추정값(estimate)**  
> 추정량: 표본(확률변수)에 대한 함수 >> 랜덤함  
> 추정값: 실제로 관측된 데이터($x_1, ... x_n$)을 넣어 계산한 숫자

## 데이터

데이터는 모집단으로부터 추출된 표본을 여러 확률 변수로 관측한 결과의 집합이다.  
즉, 하나의 데이터셋은 확률변수 $X_1, X_2, ..., X_n$의 실현값으로 구성되며, 각 관측치는 하나의 표본에 해당한다.

데이터의 종류에는 연속형 데이터와 범주형 데이터가 있다.  
범주형 데이터는 엄마, 아빠, 형, 누나와 같이 순서를 가지지 않는 데이터를 의미한다.

### 데이터 전처리

데이터 전처리는 원시 데이터를 분석 및 학습에 적합한 형태로 변환하는 과정이다.  
표본은 범주형 변수, 서로 다른 스케일 등을 포함하며, 측정상의 오류 등으로 인해 이상치와 결측치가 데이터에 흘러들어올 수 있다.

이를 그대로 사용하면 통계적 추정 및 머신러닝 학습에 문제가 발생할 수 있다.  
즉, 적절한 처리 과정이 필요하다.

### 결측치

결측치를 적절히 처리하지 않고 머신러닝 학습을 수행할 경우, 분석 결과가 잘못되거나 산술연산 등의 데이터 분석 시 에러가 발생할 수 있다.  
이는 결측치의 특성이 0, NaN이기 때문이다.

데이터 분석 시 결측치 문제를 해결하기 위해서는 결측치 발생 사유를 이해하는 것이 중요하다.  
결측치 발생의 주요 원인은 다음과 같다.

- 사람들이 설문조사(혹은 특정 질문)에 응답하지 않음
- 목표로 한 대상이 희귀하여 샘플링되지 못함
- 샘플링되기 전에 대상이 없어짐(샘플링을 위한 정보가 최신이 아님)
- 어떤 것이 다른 것보다 측정이 쉬운 경우

#### 결측치 종류

##### 완전 무작위 결측

완전 무작위 결측(MCAR, Missing Completely At Random)은 결측치가 완전히 무작위하게 발생하며, 결측이 특정 변수와 연관성이 없다.  
즉, 변수의 관측치와 결측치에 모두 독립이다.  
또한, 이런 무작위성에 의거하여 결측치 대체 전후의 분포 변화가 없다.  
→ 관측치가 많다면 아얘 지워버리는 것도 방법이다.

해결을 위해 데이터 수집 상황을 총체적으로 점검이 필요하다.

> 예시.  
> 센서 오류, TCP 통신 중 데이터 누락

##### 무작위 결측

무작위 결측(MAR, Missing At Random)은 결측치는 무작위하게 발생하지만, 결측 여부가 관측값이 존재하는 특정 변수에 연관이 되어있는 경우이다.  
정리하자면 다음과 같다.

- 결측치는 관측된 특정 변수로 추정 가능
- 특정 변수값에 따른 조건부 발생 가능
- 다양한 결측치 대체 기법으로 추정 가능

> 예시.  
> 해시계(밤이 되면 측정 불가)

##### 비 무작위 결측

비 무작위 결측(MNAR, Missing Not At Random)은 결측치가 무작위하게 발생하지 않는다.  
MCAR, MAR 외의 모든 경우에 해당하며, 관측된 변수 혹은 관측되지 않은 변수(결측값)에 연관된다.

결측값에도 영향을 받는다는 특성때문에 결측치 원인을 특정짓기가 어렵고, 단순 결측치 대체법만으로 해결이 어렵다.

> 예시.  
> 만족도 설문조사에 불만족하여 응답하지 않음.

##### 동일한 예시로 정리

![image](./img/결측치%20종류.png)

Source: https://x.com/rlmcelreath/status/1101435108995805185

#### 결측치 대체 방법

##### 단순 대치법

단순 대치법(single imputation)은 결측치를 한 번만 대체하여 하나의 완성된 데이터셋을 만드는 모든 방법을 뜻한다.

###### 단순 삭제

결측치가 포함된 모든 관측치를 제거하여 완전 관측 데이터만 사용하는 방법으로,  
단순 대치법의 가장 극단적 형태이다.

###### 평균 대치법

평균 대치법은 결측치를 해당 변수의 평균, 중앙값 또는 최빈값으로 대체하는 방법이다.
연속형 변수의 경우 평균이나 중앙값을, 범주형 변수의 경우 최빈값을 주로 사용한다.

구현이 매우 간단하며, 결측치를 포함한 관측치를 제거하지 않아 데이터 손실이 없다.  
하지만 모든 결측치를 동일한 값으로 대체하기 때문에 데이터의 분산을 과소추정하게 되며, 실제 분포를 왜곡할 가능성이 높다.  
이러한 이유 때문에 대게 결측 비율이 낮은 경우에 한해 사용한다.

###### 단순 확률 대치법

단순 확률 대치법(single stochastic imputation)은 결측치를 확률적으로 대체하는 단순 대치법의 한 종류이다.
결측치가 포함된 관측치와 가장 유사한 관측치들을 활용하여 값을 대체한다.

대표적인 방법으로 k-최근접 이웃(k-Nearest Neighbors, k-NN) 대치법이 있다.
이는

- 결측치를 포함한 관측치와 가장 가까운 k개의 이웃을 탐색하여
- 연속형 변수는 평균 또는 중앙 값으로
- 범주형 변수는 최빈값으로 대체하는 방법이다.

평균 대치법에 비해 보다 현실적인 값을 생성할 수 있다.

##### 다중 대치법

다중 대치법(multiple imutation)은 결측치를 여러번 대치하여 여러개의 독립적인 완성 데이터셋을 생성한 뒤 이를 활용하는 방법이다.  
절차는 일반적으로 다음과 같다.

1. 결측치를 서로 다른 방식으로 여러 번 대치
2. 각 데이터 셋에 대하여 분석 수행
3. 결과를 통합하여 최종 추론

다중 대치법은 단순 대치법에 비해 결측치로 인한 불확실성을 분석에 반영할 수 있으며, 추정 결과의 편향을 줄일 수 있다는 장점이 있다.

### 데이터 분포 변형 방법론

#### 표준화

표준화(standardization)는 데이터를 평균 0, 분산 1을 가지는 분포로 변환하는 방법이다.

$$z=\frac{x_i-mean(x)}{stddev(x)}$$

정규분포 가정을 활용하는 모델(선형 회귀, 로지스틱 회귀 등)에 효과적인 전처리 방법이다.

#### 정규화

정규화(normalization)는 데이터의 값을 일정한 범위로 변환하여 변수 간 상대적 크기의 영향을 줄이는 방법이다.  
분포 가정이 필요 없는 알고리즘에서 주로 사용한다.

##### MinMax scaler

데이터를 $[0,1]$ 구간으로 변환하는 방법이다

$$ x_i = \frac{x_i - min(x)}{max(x)-min(x)}$$

데이터의 상대적 크기를 유지할 수 있다는 장점이 있으나, 이상치에 매우 민감하다.

##### Robust scaler

중앙값과 사분위수 (Q1, Q3)를 이용해 정규화하는 방법이다.

$$x_i = \frac{x_i-Q1(x)}{Q3(x)-Q1(x)}$$

이상치의 영향을 줄일 수 있어 분포에 극단값이 존재하는 경우 적합하다.

##### Max absolute value scaler

각 데이터를 최대 절댓값으로 나누어 정규화하는 방법이다.

$$x_i = \frac{x_i}{max|x|}$$

##### 기타 변환 방법

분포의 왜도(skewness)를 줄이거나  
비선형 관계를 선형 관계로 완화하기 위해 다음과 같은 변환을 사용한다.

간략하게만 적고 넘어가겠다.

###### Log transformation

$$x_i = log(x_i)$$

큰 값을 압축하여 분포의 치우침을 완화한다.

###### Inverse transformation

$$x_i = \frac{1}{x_i + a}$$

큰 값의 영향을 줄이고 분포를 안정화한다.

###### Square Root transformation

$$x_i = \sqrt{x_i}$$

로그 변환보다 완만하게 분포를 조정한다.

###### Box-Cox transformation

$$x'_i = \begin{cases} \frac{x_i^\lambda - 1}{\lambda}, & \lambda \neq 0 \\ \log(x_i), & \lambda = 0 \end{cases}$$

분포의 정규성을 개선하기 위한 일반적인 변환 방법이다.

#### 범주형 데이터 처리

범주형 데이터는 값 자체에 크고 작음의 의미를 지니지 않는 데이터이다.  
이를 머신러닝 모델에 임의로 입력하면 의미가 부여되기에, 이를 방지하기 위해 인코딩 과정이 필요하다.

##### One Hot Encoding

각 범주를 서로 독립적인 이진 변수(0,1)로 변환하는 방식이다.  
이를 통해 범주 간 크기 관계를 가정하지 않으며 해석이 직관적이나, 범주 수가 많을 경우 차원이 급격히 증가한다 (차원의 저주 문제)

##### Label Encoding

각 범주를 정수 값으로 매핑하는 방법이다.  
구현은 간단하지만, 크기 의미가 없는 범주에 적용할 경우 모델이 잘못 해석할 수 있다.

> 예시.  
> 엄마, 아빠, 동생 - X  
> 보통, 좋음, 매우 좋음 - O

##### Ordinal Encoding

데이터의 특징상 순차적 정보를 내재하고 있는 경우 사용하는 인코딩 방식이다.  
순서 정보를 보존하지만, 각 단계 간 간격이 동일하다고 가정하지는 않는다.

이 때문에 각 변수에 대한 사전 정보가 필수적이다.

##### Hashing Encoding

해시 함수를 사용하여 범주형 변수를 고정된 차원의 수치형 변수로 변환하는 방법이다.  
범주의 개수가 매우 많을 때 차원 폭발을 방지하기 위해 사용된다.

다음과 같은 특징이 있다.

- 사전에 모든 범주를 알 필요 없음
- One Hot Encoding 대비 더 적은 수의 변수 생성
- 해시 충돌 가능성 존재

해시 충돌로 인해 서로 다른 범주가 같은 값으로 매핑될 수 있으나, 대규모 다차원 데이터에서는 성능 저하가 크지 않은 경우가 많다.

##### Binary Encoding

범주를 정수로 변환한 뒤, 이를 이진수 형태로 다시 인코딩하는 방식이다.  
One Hot Encoding과 Label Encoding의 중간 성격을 가진 방법이다.

One Hot Encoding보다 차원수가 적으며, Label Encoding보다 범주간 구분이 명확하다.

### 데이터 분할

데이터 분할은 모델이 학습 데이터에만 맞춘 규칙을 찾는 것이 아니라, 새로운 데이터에도 적용되는 규칙을 찾아는지를 확인하기 위하여 필요하다.  
머신러닝에서 우리가 진짜 원하는건 훈련 데이터(이미 아는 관측치)에 대한 추론을 잘 수행하는 것이 아닌, 새로운 데이터(모르는 관측치)에 대한 추론을 잘 수행하는 것이기 때문에 학습용 데이터와 평가용 데이터를 분리하게 된다.

이를 보다 자세하게 설명하면 다음과 같다.

1. 일반화 성능을 공정하게 평가하기 위하여 분리한다.  
   학습에 사용된 데이터를 다시 평가에 사용하면, 모델이 그 데이터를 이미 보았기 때문에 성능이 과대평가된다.
2. 과적합(overfitting)을 탐지하기 위해
   모델은 학습데이터의 노이즈 및 이상치까지 정확하게 적합시킬 수 있다. 이 경우 학습 성능은 높지만 일반화 성능(테스트 성능)이 떨어지게 된다.  
   이러한 특성을 활용하여 학습/테스트 성능 차이를 비교하면 과적합 여부를 판단할 수 있다.

또한, 스케일링, 결측치 대치, 인코딩 등을 데이터 전체에 대해 먼저 수행하게 되면, 테스트 정보가 학습 시에 간접적으로 노출되는 데이터 누수(data leakage)가 발생하게 된다.  
반드시 분할을 먼저 하고, 학습 데이터로만 전처리를 하도록 하자.

### 샘플링 편향
